# RAG Architecture Research - Chofetz Chaim Chatbot

**Date**: December 16, 2024  
**Purpose**: Choose best RAG architecture for our stack (Node.js 22, tRPC, Drizzle, Manus LLM)

---

## Executive Summary

After researching RAG best practices, I recommend a **simple, production-ready approach** that leverages our existing stack without adding complex dependencies:

**Recommended Architecture**:
- âœ… **Embeddings**: Manus LLM API (already available, no setup needed)
- âœ… **Storage**: MySQL JSON column (no separate vector DB needed)
- âœ… **Search**: Cosine similarity in JavaScript (simple, fast)
- âœ… **Chunking**: Semantic (by ×›×œ×œ + ×¡×¢×™×£)
- âœ… **Context**: Top 3-5 relevant chunks

**Why this approach?**
1. **Zero additional infrastructure** - uses what we have
2. **Fast development** - 1-2 days vs 1-2 weeks
3. **Good enough accuracy** - for ~200 chunks, in-memory search is fine
4. **Easy to upgrade** - can add pgvector later if needed

---

## 1. RAG Architecture Options

### Option A: Simple RAG (Recommended â­)

```
User Query
    â†“
Generate Embedding (Manus LLM)
    â†“
Cosine Similarity Search (in MySQL or JS)
    â†“
Retrieve Top 3-5 Chunks
    â†“
Build Prompt with Context
    â†“
LLM Response (Manus LLM)
```

**Pros**:
- Simple to implement (1-2 days)
- No external dependencies
- Fast for small datasets (<10K chunks)
- Easy to debug and maintain

**Cons**:
- Not optimal for huge datasets (>100K chunks)
- No advanced features (hybrid search, reranking)

**Best for**: Our use case (Chofetz Chaim ~200-500 chunks)

---

### Option B: Vector Database RAG

```
User Query
    â†“
Generate Embedding
    â†“
Vector DB Search (pgvector/AlloyDB)
    â†“
Retrieve + Rerank
    â†“
LLM Response
```

**Pros**:
- Scales to millions of chunks
- Advanced search (hybrid, filters)
- Production-grade

**Cons**:
- Requires AlloyDB or pgvector setup
- More complex (3-5 days)
- Overkill for small datasets

**Best for**: Large-scale applications (>10K chunks)

---

### Option C: LangChain RAG

```
LangChain
    â†“
Document Loaders
    â†“
Text Splitters
    â†“
Vector Store
    â†“
Retrieval Chain
```

**Pros**:
- Batteries included
- Many integrations

**Cons**:
- Heavy dependency (~50MB)
- Black box behavior
- Harder to customize

**Not recommended**: We want full control

---

## 2. Embedding Strategy

### Manus LLM API (Recommended â­)

```typescript
import { invokeLLM } from './server/_core/llm';

async function generateEmbedding(text: string): Promise<number[]> {
  const response = await invokeLLM({
    model: 'text-embedding-3-small', // or whatever Manus supports
    input: text,
  });
  
  return response.data[0].embedding; // 1536-dimensional vector
}
```

**Pros**:
- Already available (no setup)
- Free (included in Manus)
- Fast (API call)

**Cons**:
- Requires internet (not offline)

---

### Alternative: Vertex AI Embeddings

**Pros**:
- Google Cloud native
- Scalable

**Cons**:
- Requires GCP setup
- Costs money (~$0.0001/1K tokens)
- More complex

**Not recommended**: Manus LLM is simpler

---

## 3. Storage Strategy

### Option A: MySQL JSON Column (Recommended â­)

```typescript
export const chofetzChaimContent = mysqlTable("chofetz_chaim_content", {
  id: int("id").primaryKey(),
  content: text("content"),
  embedding: json("embedding").$type<number[]>(), // Store as JSON array
});
```

**Search**:
```typescript
// Load all embeddings into memory (fast for <10K)
const allContent = await db.select().from(chofetzChaimContent);

// Compute cosine similarity in JS
const results = allContent
  .map(item => ({
    ...item,
    score: cosineSimilarity(queryEmbedding, item.embedding),
  }))
  .sort((a, b) => b.score - a.score)
  .slice(0, 5);
```

**Pros**:
- No additional infrastructure
- Fast for small datasets
- Easy to implement

**Cons**:
- Loads all data into memory
- Not optimal for >10K chunks

**Best for**: Our use case (~200-500 chunks)

---

### Option B: pgvector Extension

```sql
CREATE EXTENSION vector;

CREATE TABLE chofetz_chaim_content (
  id SERIAL PRIMARY KEY,
  content TEXT,
  embedding vector(1536)
);

CREATE INDEX ON chofetz_chaim_content USING ivfflat (embedding vector_cosine_ops);
```

**Search**:
```sql
SELECT * FROM chofetz_chaim_content
ORDER BY embedding <=> $1
LIMIT 5;
```

**Pros**:
- Optimized for vector search
- Scales to millions

**Cons**:
- Requires PostgreSQL (we use MySQL)
- Or requires AlloyDB (expensive)

**Not recommended**: Overkill for our scale

---

## 4. Chunking Strategy

### Semantic Chunking (Recommended â­)

**Strategy**: Chunk by natural boundaries (×›×œ×œ + ×¡×¢×™×£)

```typescript
interface Chunk {
  id: string;
  book: 'chofetz_chaim' | 'shmirat_halashon';
  section: 'lashon_hara' | 'rechilut';
  klal: number;        // 1-10
  seif: number;        // 1-20
  content: string;     // Full text of this seif
  embedding: number[]; // 1536-dim vector
}
```

**Example**:
- Chunk 1: ×›×œ×œ ×' ×¡×¢×™×£ ×' (full text)
- Chunk 2: ×›×œ×œ ×' ×¡×¢×™×£ ×‘' (full text)
- Chunk 3: ×›×œ×œ ×' ×‘××¨ ××™× ×—×™×™× ×¡×¢×™×£ ×' (commentary)

**Pros**:
- Preserves semantic meaning
- Natural boundaries
- Easy to cite sources

**Cons**:
- Variable chunk sizes

**Best for**: Structured religious texts

---

### Fixed-Size Chunking

**Strategy**: Split by character count (e.g., 500 chars)

**Pros**:
- Uniform size
- Simple

**Cons**:
- Breaks semantic units
- Hard to cite sources

**Not recommended**: Loses structure

---

## 5. Context Window Strategy

### Top-K Retrieval (Recommended â­)

```typescript
const TOP_K = 5; // Retrieve top 5 most relevant chunks

const context = topChunks
  .map(chunk => `
[${chunk.klal}:${chunk.seif}]
${chunk.content}
  `)
  .join('\n\n');

const prompt = `
××ª×” ××•××—×” ×œ×”×œ×›×•×ª ×œ×©×•×Ÿ ×”×¨×¢ ×¢×œ ×¤×™ ×¡×¤×¨ ×—×¤×¥ ×—×™×™×.
×¢× ×” ×¢×œ ×”×©××œ×” ×”×‘××” ×‘×”×ª×‘×¡×¡ ×¢×œ ×”××§×•×¨×•×ª ×©×œ×”×œ×Ÿ ×‘×œ×‘×“.
×¦×™×™×Ÿ ×ª××™×“ ××ª ×”××§×•×¨ (×›×œ×œ, ×¡×¢×™×£).

××§×•×¨×•×ª:
${context}

×©××œ×”: ${userQuestion}

×ª×©×•×‘×”:
`;
```

**Pros**:
- Simple and effective
- Fits in context window
- Cites sources

**Cons**:
- May miss relevant chunks

**Best for**: Most use cases

---

### Hybrid Search

**Strategy**: Combine semantic + keyword search

**Pros**:
- More accurate

**Cons**:
- More complex

**Not recommended**: Simple is better for v1

---

## 6. Recommended Architecture

### Stack

```
Frontend (React)
    â†“
tRPC API
    â†“
Embedding Generation (Manus LLM)
    â†“
Cosine Similarity Search (JavaScript)
    â†“
Context Building
    â†“
LLM Response (Manus LLM)
    â†“
Stream to Frontend
```

### Database Schema

```typescript
export const chofetzChaimContent = mysqlTable("chofetz_chaim_content", {
  id: int("id").primaryKey(),
  book: mysqlEnum("book", ["chofetz_chaim", "shmirat_halashon"]),
  section: mysqlEnum("section", ["lashon_hara", "rechilut"]),
  klal: int("klal"),
  seif: int("seif"),
  title: varchar("title", { length: 300 }),
  content: text("content"),
  embedding: json("embedding").$type<number[]>(),
  createdAt: timestamp("createdAt").defaultNow(),
});
```

### Cosine Similarity Function

```typescript
function cosineSimilarity(a: number[], b: number[]): number {
  const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const magnitudeA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const magnitudeB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dotProduct / (magnitudeA * magnitudeB);
}
```

### RAG Endpoint

```typescript
export const chofetzChaimRouter = router({
  ask: publicProcedure
    .input(z.object({
      question: z.string(),
      conversationId: z.number().optional(),
    }))
    .mutation(async ({ input }) => {
      // 1. Generate embedding for question
      const questionEmbedding = await generateEmbedding(input.question);
      
      // 2. Search for relevant chunks
      const allContent = await db.select().from(chofetzChaimContent);
      const topChunks = allContent
        .map(chunk => ({
          ...chunk,
          score: cosineSimilarity(questionEmbedding, chunk.embedding),
        }))
        .sort((a, b) => b.score - a.score)
        .slice(0, 5);
      
      // 3. Build context
      const context = topChunks
        .map(c => `[×›×œ×œ ${c.klal} ×¡×¢×™×£ ${c.seif}]\n${c.content}`)
        .join('\n\n');
      
      // 4. Generate response
      const response = await invokeLLM({
        messages: [
          {
            role: 'system',
            content: '××ª×” ××•××—×” ×œ×”×œ×›×•×ª ×œ×©×•×Ÿ ×”×¨×¢ ×¢×œ ×¤×™ ×¡×¤×¨ ×—×¤×¥ ×—×™×™×...',
          },
          {
            role: 'user',
            content: `××§×•×¨×•×ª:\n${context}\n\n×©××œ×”: ${input.question}`,
          },
        ],
      });
      
      return {
        answer: response.choices[0].message.content,
        sources: topChunks.map(c => ({
          klal: c.klal,
          seif: c.seif,
          title: c.title,
        })),
      };
    }),
});
```

---

## 7. Implementation Plan

### Day 1: Scraping & Parsing (4-6 hours)
1. âœ… Scrape Wikisource using MediaWiki API
2. âœ… Parse HTML structure (×›×œ×œ×™×, ×¡×¢×™×¤×™×)
3. âœ… Extract clean text
4. âœ… Save to JSON files

### Day 2: Database Population (3-4 hours)
1. âœ… Populate chofetz_chaim_content table
2. âœ… Generate embeddings for all chunks
3. âœ… Store embeddings in JSON column
4. âœ… Verify data integrity

### Day 3: RAG Implementation (4-6 hours)
1. âœ… Implement cosine similarity function
2. âœ… Build RAG endpoint
3. âœ… Test with sample questions
4. âœ… Optimize performance

### Day 4: UI & Testing (3-4 hours)
1. âœ… Build chat UI component
2. âœ… Add streaming support
3. âœ… Write tests
4. âœ… Deploy to production

**Total**: 14-20 hours (2-3 days)

---

## 8. Cost Estimation

### Embeddings Generation (One-time)
- ~500 chunks Ã— 200 tokens/chunk = 100K tokens
- Cost: Free (Manus LLM)

### Query Embeddings (Ongoing)
- ~100 queries/day Ã— 20 tokens/query = 2K tokens/day
- Cost: Free (Manus LLM)

### LLM Responses (Ongoing)
- ~100 queries/day Ã— 500 tokens/response = 50K tokens/day
- Cost: Free (Manus LLM)

**Total Monthly Cost**: $0 (using Manus LLM)

---

## 9. Performance Expectations

### Search Speed
- In-memory cosine similarity: **<50ms** for 500 chunks
- Embedding generation: **~200ms** per query
- LLM response: **~2-5s** per query

**Total latency**: **~3-6s** per query (acceptable for chatbot)

### Accuracy
- Expected: **80-90%** relevant chunks in top-5
- Good enough for v1, can improve with:
  - Better chunking
  - Reranking
  - Hybrid search

---

## 10. Future Improvements (v2)

If we need better performance/accuracy:

1. **Add pgvector** (if we migrate to PostgreSQL)
2. **Hybrid search** (semantic + keyword)
3. **Reranking** (use LLM to rerank top-20 â†’ top-5)
4. **Fine-tuned embeddings** (train on Chofetz Chaim)
5. **Conversation memory** (remember previous questions)

---

## Conclusion

**Recommended Approach**: Simple RAG with Manus LLM

**Why**:
- âœ… Fast to implement (2-3 days)
- âœ… Zero additional cost
- âœ… Good enough accuracy
- âœ… Easy to maintain
- âœ… Can upgrade later if needed

**Next Steps**:
1. Scrape Wikisource content
2. Populate database with embeddings
3. Build RAG endpoint
4. Test and deploy

Let's build it! ğŸš€
